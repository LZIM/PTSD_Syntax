{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ml/lib/python3.5/site-packages/nltk/corpus/reader/wordlist.py:25: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/pvomelveny/nltk_data/corpora/stopwords/english'>\n",
      "  return concat([self.open(f).read() for f in fileids])\n"
     ]
    }
   ],
   "source": [
    "stops_w_pronouns = set(stopwords.words('english')) - set(['i', 'he', 'she', 'you', \n",
    "                                       'me', 'him', 'her', 'yours',\n",
    "                                       'mine', 'his', 'hers', 'ours', 'our', 'your',\n",
    "                                       'hiself', 'herself', 'myself', 'ourselves', 'yourselves',\n",
    "                                       'yourself', 'it', 'itself', 'them', 'themselves', 'its',\n",
    "                                       'themselves', 'their', 'theirselves', 'they'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops_w_pronouns.add('ptsd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'don',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'here',\n",
       " 'himself',\n",
       " 'how',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'just',\n",
       " 'more',\n",
       " 'most',\n",
       " 'my',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'ptsd',\n",
       " 's',\n",
       " 'same',\n",
       " 'should',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'theirs',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops = stops_w_pronouns\n",
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pickeled reddit data from Seb's reddit scraper script ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Reddit/reddit_data.p', 'rb') as f:\n",
    "    reddit_data = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    # Replace non-letter characters w/ space\n",
    "    letters_only = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Lower\n",
    "    low_string = letters_only.lower()\n",
    "    # Split\n",
    "    words = [thing for thing in low_string.split(' ') if thing]\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in words]\n",
    "    \n",
    "    meaningful_words = [word for word in lemmas if word not in stops_w_pronouns]\n",
    "    \n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reddit_data['clean'] = reddit_data['text'].apply(cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out variables ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, stop_words=None, max_features=5000)\n",
    "\n",
    "preds = pd.DataFrame(vectorizer.fit_transform(reddit_data['clean']).toarray(), index=reddit_data.index)\n",
    "\n",
    "labels = LabelEncoder().fit(reddit_data['flag'])\n",
    "target = pd.Series(labels.transform(reddit_data['flag']), index=reddit_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost on our cleaned data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/ml/lib/python3.5/site-packages/xgboost/training.py:272: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  idset = [randidx[(i * kstep): min(len(randidx), (i + 1) * kstep)] for i in range(nfold)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.943148</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.961346</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.006727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>0.011467</td>\n",
       "      <td>0.975456</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.067375</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972123</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.985033</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.057375</td>\n",
       "      <td>0.005440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.975958</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.0780</td>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.987691</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.049125</td>\n",
       "      <td>0.008068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978725</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.016926</td>\n",
       "      <td>0.989843</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.047625</td>\n",
       "      <td>0.006828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test-auc-mean  test-auc-std  test-error-mean  test-error-std  \\\n",
       "0       0.943148      0.013296           0.1025        0.011726   \n",
       "1       0.961435      0.011535           0.0960        0.011467   \n",
       "2       0.972123      0.009468           0.0870        0.012186   \n",
       "3       0.975958      0.008453           0.0780        0.016688   \n",
       "4       0.978725      0.006595           0.0735        0.016926   \n",
       "\n",
       "   train-auc-mean  train-auc-std  train-error-mean  train-error-std  \n",
       "0        0.961346       0.006236          0.071000         0.006727  \n",
       "1        0.975456       0.003421          0.067375         0.005265  \n",
       "2        0.985033       0.004068          0.057375         0.005440  \n",
       "3        0.987691       0.003983          0.049125         0.008068  \n",
       "4        0.989843       0.002518          0.047625         0.006828  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(preds.values, target.values)\n",
    "\n",
    "xgboost_params = {'objective': 'binary:logistic', 'booster': 'gbtree', 'eval_metric': 'auc', 'eta': 0.01,\n",
    "                  'subsample': 0.75, 'colsample_bytree': 0.68, 'max_depth': 7}\n",
    "\n",
    "xgb.cv(xgboost_params, xgtrain, num_boost_round=5, nfold=5, metrics={'error'}, seed=0, show_stdv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
